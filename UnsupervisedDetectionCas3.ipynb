{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)-Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>78</td><td>application_1576053140655_0079</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://spark-master.local:8088/proxy/application_1576053140655_0079/\">Link</a></td><td><a target=\"_blank\" href=\"http://spark-worker01.local:8042/node/containerlogs/container_1576053140655_0079_01_000001/sparkuser\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark._\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n",
      "import org.apache.spark.ml.clustering._\n",
      "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
      "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}\n",
      "import org.apache.spark.ml.feature.{StandardScaler, VectorAssembler, StringIndexer, MinMaxScaler}\n",
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
      "import java.io.{File, PrintWriter}\n",
      "import java.text.SimpleDateFormat\n",
      "import java.util.Calendar\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n",
    "//import org.apache.spark.ml.clustering.{KMeans, KMeansModel, GaussianMixture, GaussianMixtureModel, BisectingKMeans, BisectingKMeansModel}\n",
    "import org.apache.spark.ml.clustering._\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator,BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.feature.{StandardScaler, VectorAssembler, StringIndexer, MinMaxScaler}\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import java.io.{File, PrintWriter}\n",
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)-Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: org.apache.spark.sql.DataFrame = [Time: string, V1: string ... 29 more fields]\n"
     ]
    }
   ],
   "source": [
    "val raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv(\"datasets/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 29 more fields]\n"
     ]
    }
   ],
   "source": [
    "// cast all the column to Double type.\n",
    "val df = raw.select(((1 to 28).map(i => \"V\" + i) ++ Array(\"Time\", \"Amount\", \"Class\")).map(s => col(s).cast(\"Double\")): _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)-Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelConverter: org.apache.spark.ml.feature.StringIndexer = strIdx_805a99f48742\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_9d9e732dd7fe\n",
      "scaler: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_992322055318\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_71b17bd65f89\n",
      "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_71b17bd65f89\n",
      "data: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 32 more fields]\n",
      "Generate feature from raw data:\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.88136490328633...|  0.0|\n",
      "|[0.84029849039390...|  0.0|\n",
      "|[0.86814081926190...|  0.0|\n",
      "|[0.86848364774806...|  0.0|\n",
      "|[0.86425070140685...|  0.0|\n",
      "|[0.85718742663956...|  0.0|\n",
      "|[0.83819983804773...|  0.0|\n",
      "|[0.85603110871019...|  0.0|\n",
      "|[0.83545216735689...|  0.0|\n",
      "|[0.85551101187934...|  0.0|\n",
      "|[0.85324951634142...|  0.0|\n",
      "|[0.82226325870284...|  0.0|\n",
      "|[0.84406658980857...|  0.0|\n",
      "|[0.85177230541965...|  0.0|\n",
      "|[0.86586281889129...|  0.0|\n",
      "|[0.87306410273210...|  0.0|\n",
      "|[0.85937468703684...|  0.0|\n",
      "|[0.85343546827309...|  0.0|\n",
      "|[0.85797059662233...|  0.0|\n",
      "|[0.84529457208090...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val labelConverter = new StringIndexer().setInputCol(\"Class\").setOutputCol(\"label\")\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"V3\", \"V4\", \"V9\", \"V10\", \"V11\", \"V12\", \"V14\", \"V16\", \"V17\", \"V18\",\"V19\")).setOutputCol(\"assembled\")\n",
    "val scaler = new MinMaxScaler().setInputCol(\"assembled\").setOutputCol(\"features\")\n",
    "//val scaler = new StandardScaler().setInputCol(\"assembled\").setOutputCol(\"features\")\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, scaler, labelConverter))\n",
    "val pipelineModel = pipeline.fit(df)\n",
    "val data = pipelineModel.transform(df)\n",
    "println(\"Generate feature from raw data:\")\n",
    "data.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "data1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n"
     ]
    }
   ],
   "source": [
    "val data0 = data.filter($\"Class\" === \"0\").cache()\n",
    "val data1 = data.filter($\"Class\" === \"1\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitTime0: Double = 132934.0\n",
      "splitTime1: Double = 93824.0\n",
      "trainingData0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "validationData0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "trainingData1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "validationData1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n"
     ]
    }
   ],
   "source": [
    "val splitTime0 = data0.stat.approxQuantile(\"Time\", Array(0.7), 0.001).head\n",
    "val splitTime1 = data1.stat.approxQuantile(\"Time\", Array(0.6), 0.001).head\n",
    "val trainingData0 = data0.filter(s\"Time<$splitTime0\").cache()\n",
    "val validationData0 = data0.filter(s\"Time>=$splitTime0\").cache()\n",
    "val trainingData1 = data1.filter(s\"Time<$splitTime1\").cache()\n",
    "val validationData1 = data1.filter(s\"Time>=$splitTime1\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "validationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n"
     ]
    }
   ],
   "source": [
    "val trainingData = trainingData0.unionAll(trainingData1)\n",
    "val validationData = validationData0.unionAll(validationData1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training set statistics: 1 represents fraud and 0 represents normal\n",
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|  0.0|198993|\n",
      "|  1.0|   295|\n",
      "+-----+------+\n",
      "\n",
      " validation set statistics: 1 represents fraud and 0 represents normal\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|  0.0|85322|\n",
      "|  1.0|  197|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\" Training set statistics: 1 represents fraud and 0 represents normal\")\n",
    "trainingData.groupBy(\"Class\").count().show()\n",
    "println(\" validation set statistics: 1 represents fraud and 0 represents normal\")\n",
    "validationData.groupBy(\"Class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)-Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_ff1415de2ad0\n",
      "bisectingkmeans: org.apache.spark.ml.clustering.BisectingKMeans = bisecting-kmeans_19bffe496895\n",
      "gaussianMixture: org.apache.spark.ml.clustering.GaussianMixture = GaussianMixture_c33ee33bcb7f\n"
     ]
    }
   ],
   "source": [
    "// Create a Kmeans Model with K=2\n",
    "// train the model\n",
    "val kmeans = new KMeans().setK(2).setFeaturesCol(\"features\").setPredictionCol(\"clusters\").setSeed(1L).setMaxIter(100)\n",
    "val bisectingkmeans = new BisectingKMeans().setK(2).setFeaturesCol(\"features\").setPredictionCol(\"clusters\").setSeed(1L).setMaxIter(100)\n",
    "val gaussianMixture = new GaussianMixture().setK(2).setFeaturesCol(\"features\").setPredictionCol(\"clusters\").setSeed(1L).setMaxIter(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 358400929306774\n",
      "modelKmeans: org.apache.spark.ml.clustering.KMeansModel = kmeans_ff1415de2ad0\n",
      "durationkmeans: Double = 9.859007144\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 9.859007144 secs\n"
     ]
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val t = System.nanoTime\n",
    "val modelKmeans = kmeans.fit(trainingData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 358412208709298\n",
      "modelBisectingkmeans: org.apache.spark.ml.clustering.BisectingKMeansModel = bisecting-kmeans_19bffe496895\n",
      "durationbisectingkmeans: Double = 37.809269994\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 37.809269994 secs\n"
     ]
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val t = System.nanoTime\n",
    "val modelBisectingkmeans = bisectingkmeans.fit(trainingData)\n",
    "val durationbisectingkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationbisectingkmeans secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 358451818381165\n",
      "modelGMM: org.apache.spark.ml.clustering.GaussianMixtureModel = GaussianMixture_c33ee33bcb7f\n",
      "durationGMM: Double = 10.75590278\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 10.75590278 secs\n"
     ]
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val t = System.nanoTime\n",
    "val modelGMM = gaussianMixture.fit(trainingData)\n",
    "val durationGMM = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationGMM secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)- Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 358463390988614\n",
      "predictionsk: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 33 more fields]\n",
      "durationkmeans: Double = 0.246136773\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 0.246136773 secs\n",
      "predictionsk: org.apache.spark.sql.DataFrame = [label: double, clusters: double ... 1 more field]\n",
      "+--------+-----+\n",
      "|clusters|count|\n",
      "+--------+-----+\n",
      "|     0.0|48514|\n",
      "|     1.0|37005|\n",
      "+--------+-----+\n",
      "\n",
      "+-----+--------+--------------------+\n",
      "|label|clusters|            features|\n",
      "+-----+--------+--------------------+\n",
      "|  0.0|     1.0|[0.83928824494586...|\n",
      "|  0.0|     1.0|[0.84019343593776...|\n",
      "|  0.0|     1.0|[0.78506093453763...|\n",
      "|  0.0|     1.0|[0.86327731276444...|\n",
      "|  0.0|     1.0|[0.81905350742297...|\n",
      "|  0.0|     0.0|[0.80576713299952...|\n",
      "|  0.0|     0.0|[0.75665670211160...|\n",
      "|  0.0|     1.0|[0.81390257419240...|\n",
      "|  0.0|     1.0|[0.82786727703355...|\n",
      "|  0.0|     0.0|[0.84443102534225...|\n",
      "+-----+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " // Make predictions Kmeans\n",
    "val t = System.nanoTime\n",
    "var predictionsk = modelKmeans.transform(validationData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")\n",
    "predictionsk = predictionsk.select(col(\"label\"),col(\"clusters\").cast(\"Double\"),col(\"features\"))\n",
    "//val df = raw.select(((1 to 28).map(i => \"V\" + i) ++ Array(\"Time\", \"Amount\", \"Class\")).map(s => col(s).cast(\"Double\")): _*)\n",
    "predictionsk.groupBy(\"clusters\").count().show()\n",
    "//predictionsk.groupBy(\"label\").count().show()\n",
    "predictionsk.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|clusters|label|count|\n",
      "+--------+-----+-----+\n",
      "|     0.0|  0.0|48476|\n",
      "|     1.0|  0.0|36846|\n",
      "|     0.0|  1.0|   38|\n",
      "|     1.0|  1.0|  159|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsk.select(\"clusters\", \"label\").groupBy(\"clusters\", \"label\").count().orderBy(\"label\", \"clusters\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 358467894947030\n",
      "predictionsbk: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 33 more fields]\n",
      "durationkmeans: Double = 0.235729777\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 0.235729777 secs\n",
      "predictionsbk: org.apache.spark.sql.DataFrame = [label: double, clusters: double ... 1 more field]\n",
      "+--------+-----+\n",
      "|clusters|count|\n",
      "+--------+-----+\n",
      "|     0.0|48511|\n",
      "|     1.0|37008|\n",
      "+--------+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|85322|\n",
      "|  1.0|  197|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " // Make predictions Bisecting Kmeans \n",
    "val t = System.nanoTime\n",
    "var predictionsbk = modelBisectingkmeans.transform(validationData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")\n",
    "predictionsbk = predictionsbk.select(col(\"label\"),col(\"clusters\").cast(\"Double\"),col(\"features\"))\n",
    "predictionsbk.groupBy(\"clusters\").count().show()\n",
    "predictionsbk.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|clusters|label|count|\n",
      "+--------+-----+-----+\n",
      "|     0.0|  0.0|48473|\n",
      "|     1.0|  0.0|36849|\n",
      "|     0.0|  1.0|   38|\n",
      "|     1.0|  1.0|  159|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsbk.select(\"clusters\", \"label\").groupBy(\"clusters\", \"label\").count().orderBy(\"label\", \"clusters\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 358472399412990\n",
      "predictionsgmm: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 34 more fields]\n",
      "durationkmeans: Double = 0.237759191\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 0.237759191 secs\n",
      "predictionsgmm: org.apache.spark.sql.DataFrame = [label: double, clusters: double ... 1 more field]\n",
      "+--------+-----+\n",
      "|clusters|count|\n",
      "+--------+-----+\n",
      "|     0.0|62153|\n",
      "|     1.0|23366|\n",
      "+--------+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|85322|\n",
      "|  1.0|  197|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Make predictions Bisecting Kmeans \n",
    "val t = System.nanoTime\n",
    "var predictionsgmm = modelGMM.transform(validationData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")\n",
    "predictionsgmm = predictionsgmm.select(col(\"label\"),col(\"clusters\").cast(\"Double\"),col(\"features\"))\n",
    "predictionsgmm.groupBy(\"clusters\").count().show()\n",
    "predictionsgmm.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|clusters|label|count|\n",
      "+--------+-----+-----+\n",
      "|     0.0|  0.0|62105|\n",
      "|     1.0|  0.0|23217|\n",
      "|     0.0|  1.0|   48|\n",
      "|     1.0|  1.0|  149|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsgmm.select(\"clusters\", \"label\").groupBy(\"clusters\", \"label\").count().orderBy(\"label\", \"clusters\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)-Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Calculation of Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatorsi: org.apache.spark.ml.evaluation.ClusteringEvaluator = cluEval_557d93c04e88\n"
     ]
    }
   ],
   "source": [
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluatorsi = new ClusteringEvaluator().setPredictionCol(\"clusters\").setMetricName(\"silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreKmeans: Double = 0.24075762081495494\n",
      "Silhouette of Kmeans predictions with squared euclidean distance = 0.24075762081495494\n"
     ]
    }
   ],
   "source": [
    "val ScoreKmeans = evaluatorsi.evaluate(predictionsk)\n",
    "println(s\"Silhouette of Kmeans predictions with squared euclidean distance = $ScoreKmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreBisectingKmeans: Double = 0.24104279162878414\n",
      "Silhouette of Bisecting Kmeans predictions with squared euclidean distance = 0.24104279162878414\n"
     ]
    }
   ],
   "source": [
    "val ScoreBisectingKmeans  = evaluatorsi.evaluate(predictionsbk)\n",
    "println(s\"Silhouette of Bisecting Kmeans predictions with squared euclidean distance = $ScoreBisectingKmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreGMM: Double = 0.09193997360784865\n",
      "Silhouette of Gaussian Mixture predictions with squared euclidean distance = 0.09193997360784865\n"
     ]
    }
   ],
   "source": [
    "val ScoreGMM  = evaluatorsi.evaluate(predictionsgmm)\n",
    "println(s\"Silhouette of Gaussian Mixture predictions with squared euclidean distance = $ScoreGMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "costKmeans: Double = 3781.2986393374727\n",
      "Kmeans: Within Set Sum of Squared Errors = 3781.2986393374727\n",
      "costBisectingKmeans: Double = 3781.2930628987174\n",
      "Bisecting Kmeans: Within Set Sum of Squared Errors = 3781.2930628987174\n",
      "costBisectingKmeans: Double = 3781.2930628987183\n",
      "Bisecting Kmeans: Within Set Sum of Squared Errors = 3781.2930628987183\n"
     ]
    }
   ],
   "source": [
    "// Evaluate clustering.\n",
    "val costKmeans = modelKmeans.computeCost(trainingData)\n",
    "println(s\"Kmeans: Within Set Sum of Squared Errors = $costKmeans\")\n",
    "val costBisectingKmeans = modelBisectingkmeans.computeCost(trainingData)\n",
    "println(s\"Bisecting Kmeans: Within Set Sum of Squared Errors = $costBisectingKmeans\")\n",
    "val costBisectingKmeans = modelBisectingkmeans.computeCost(trainingData)\n",
    "println(s\"Bisecting Kmeans: Within Set Sum of Squared Errors = $costBisectingKmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatork1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_b7a2be46a0d5\n",
      "evaluatork2: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_7fc82fcfa9d3\n",
      "evaluatork3: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_ba2e5f8d8b9e\n",
      "evaluatork4: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_9b896a9e586c\n",
      "areaUnderROCk: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_4b6b06777912\n",
      "accuracyk: Double = 0.5687040306832399\n",
      "Area Under ROC Curve = 0.68763009094125\n",
      "Accuracy = 0.5687040306832399\n",
      "Precision = 0.9969248414712742\n",
      "Recall = 0.56870403068324\n",
      "F1 = 0.7227599337648909\n",
      "Test Error = 0.43129596931676006\n"
     ]
    }
   ],
   "source": [
    "val evaluatork1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"accuracy\")\n",
    "val evaluatork2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedPrecision\")\n",
    "val evaluatork3 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedRecall\")\n",
    "val evaluatork4 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"f1\")\n",
    "val areaUnderROCk = new BinaryClassificationEvaluator().setRawPredictionCol(\"clusters\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val accuracyk = evaluatork1.evaluate(predictionsk)\n",
    "println(\"Area Under ROC Curve = \" + areaUnderROCk.evaluate(predictionsk))\n",
    "println(\"Accuracy = \" + evaluatork1.evaluate(predictionsk))\n",
    "println(\"Precision = \" + evaluatork2.evaluate(predictionsk))\n",
    "println(\"Recall = \" + evaluatork3.evaluate(predictionsk))\n",
    "println(\"F1 = \" + evaluatork4.evaluate(predictionsk))\n",
    "println(\"Test Error = \" + (1.0 - accuracyk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatorbk1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_685793439152\n",
      "evaluatorbk2: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_97c841d9dec5\n",
      "evaluatorbk3: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_5e5afc19d259\n",
      "evaluatorbk4: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_12cc15ec6ff6\n",
      "areaUnderROCbk: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_f6d28b6cbdab\n",
      "accuracybk: Double = 0.5686689507594803\n",
      "Area Under ROC Curve = 0.6876125104813453\n",
      "Accuracy = 0.5686689507594803\n",
      "Precision = 0.9969247923412398\n",
      "Recall = 0.5686689507594803\n",
      "F1 = 0.7227314044016117\n",
      "Test Error = 0.4313310492405197\n"
     ]
    }
   ],
   "source": [
    "val evaluatorbk1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"accuracy\")\n",
    "val evaluatorbk2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedPrecision\")\n",
    "val evaluatorbk3 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedRecall\")\n",
    "val evaluatorbk4 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"f1\")\n",
    "val areaUnderROCbk = new BinaryClassificationEvaluator().setRawPredictionCol(\"clusters\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val accuracybk = evaluatorbk1.evaluate(predictionsbk)\n",
    "println(\"Area Under ROC Curve = \" + areaUnderROCbk.evaluate(predictionsbk))\n",
    "println(\"Accuracy = \" + evaluatorbk1.evaluate(predictionsbk))\n",
    "println(\"Precision = \" + evaluatorbk2.evaluate(predictionsbk))\n",
    "println(\"Recall = \" + evaluatorbk3.evaluate(predictionsbk))\n",
    "println(\"F1 = \" + evaluatorbk4.evaluate(predictionsbk))\n",
    "println(\"Test Error = \" + (1.0 - accuracybk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatorgmm1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_6c0130fc2db5\n",
      "evaluatorgmm2: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_0861cda3b105\n",
      "evaluatorgmm3: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_9f7d709fbeec\n",
      "evaluatorgmm4: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_821bec04a822\n",
      "areaUnderROCgmm: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_a0701b80d8f3\n",
      "accuracygmm: Double = 0.7279551912440511\n",
      "Area Under ROC Curve = 0.7421174096289994\n",
      "Accuracy = 0.7279551912440511\n",
      "Precision = 0.9969405990752082\n",
      "Recall = 0.7279551912440511\n",
      "F1 = 0.8403334026212206\n",
      "Test Error = 0.27204480875594894\n"
     ]
    }
   ],
   "source": [
    "val evaluatorgmm1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"accuracy\")\n",
    "val evaluatorgmm2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedPrecision\")\n",
    "val evaluatorgmm3 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedRecall\")\n",
    "val evaluatorgmm4 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"f1\")\n",
    "val areaUnderROCgmm = new BinaryClassificationEvaluator().setRawPredictionCol(\"clusters\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val accuracygmm = evaluatorgmm1.evaluate(predictionsgmm)\n",
    "println(\"Area Under ROC Curve = \" + areaUnderROCgmm.evaluate(predictionsgmm))\n",
    "println(\"Accuracy = \" + evaluatorgmm1.evaluate(predictionsgmm))\n",
    "println(\"Precision = \" + evaluatorgmm2.evaluate(predictionsgmm))\n",
    "println(\"Recall = \" + evaluatorgmm3.evaluate(predictionsgmm))\n",
    "println(\"F1 = \" + evaluatorgmm4.evaluate(predictionsgmm))\n",
    "println(\"Test Error = \" + (1.0 - accuracygmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
