{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark._\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n",
      "import org.apache.spark.ml.clustering._\n",
      "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
      "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}\n",
      "import org.apache.spark.ml.feature.{StandardScaler, VectorAssembler, StringIndexer, MinMaxScaler}\n",
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
      "import java.io.{File, PrintWriter}\n",
      "import java.text.SimpleDateFormat\n",
      "import java.util.Calendar\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n",
    "//import org.apache.spark.ml.clustering.{KMeans, KMeansModel, GaussianMixture, GaussianMixtureModel, BisectingKMeans, BisectingKMeansModel}\n",
    "import org.apache.spark.ml.clustering._\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator,BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.feature.{StandardScaler, VectorAssembler, StringIndexer, MinMaxScaler}\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import java.io.{File, PrintWriter}\n",
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: org.apache.spark.sql.DataFrame = [Time: string, V1: string ... 29 more fields]\n"
     ]
    }
   ],
   "source": [
    "val raw = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").csv(\"datasets/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 29 more fields]\n"
     ]
    }
   ],
   "source": [
    "// cast all the column to Double type.\n",
    "val df = raw.select(((1 to 28).map(i => \"V\" + i) ++ Array(\"Time\", \"Amount\", \"Class\")).map(s => col(s).cast(\"Double\")): _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelConverter: org.apache.spark.ml.feature.StringIndexer = strIdx_25c67eefc1bf\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2d989a639bb4\n",
      "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_6fc4ae5344cd\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_b9e3a65e67f1\n",
      "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_b9e3a65e67f1\n",
      "data: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 32 more fields]\n",
      "Generate feature from raw data:\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.67277056254252...|  0.0|\n",
      "|[0.10979690934882...|  0.0|\n",
      "|[1.16946643973207...|  0.0|\n",
      "|[1.18251437486170...|  0.0|\n",
      "|[1.02140988238942...|  0.0|\n",
      "|[0.75258405639254...|  0.0|\n",
      "|[0.02992291760621...|  0.0|\n",
      "|[0.70857499080745...|  0.0|\n",
      "|[-0.0746524907375...|  0.0|\n",
      "|[0.68878028300438...|  0.0|\n",
      "|[0.60270853498396...|  0.0|\n",
      "|[-0.5766178509613...|  0.0|\n",
      "|[0.25320948651198...|  0.0|\n",
      "|[0.54648639167204...|  0.0|\n",
      "|[1.08276652341442...|  0.0|\n",
      "|[1.35684492762881...|  0.0|\n",
      "|[0.83583044030665...|  0.0|\n",
      "|[0.60978580201274...|  0.0|\n",
      "|[0.78239123853538...|  0.0|\n",
      "|[0.29994607252118...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// convert the label from {0, 1} to {1, 2}\n",
    "//val labelConverter = new FuncTransformer(udf {d: Double => if (d==0) 2 else d }).setInputCol(\"Class\").setOutputCol(\"Class\")\n",
    "val labelConverter = new StringIndexer().setInputCol(\"Class\").setOutputCol(\"label\")\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"V3\", \"V4\", \"V9\", \"V10\", \"V11\", \"V12\", \"V14\", \"V16\", \"V17\", \"V18\",\"V19\")).setOutputCol(\"assembled\")\n",
    "//val scaler = new MinMaxScaler().setInputCol(\"assembled\").setOutputCol(\"features\")\n",
    "val scaler = new StandardScaler().setInputCol(\"assembled\").setOutputCol(\"features\")\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, scaler, labelConverter))\n",
    "val pipelineModel = pipeline.fit(df)\n",
    "val data = pipelineModel.transform(df)\n",
    "println(\"Generate feature from raw data:\")\n",
    "data.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitTime: Double = 132831.0\n",
      "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "validationData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [V1: double, V2: double ... 32 more fields]\n",
      "Split data into Training and Validation: \n",
      "training records count: 199155\n",
      "validation records count: 85652\n"
     ]
    }
   ],
   "source": [
    "//Splitting dataset\n",
    "val splitTime = data.stat.approxQuantile(\"Time\", Array(0.7), 0.001).head\n",
    "    val trainingData = data.filter(s\"Time<$splitTime\").cache()\n",
    "    val validationData = data.filter(s\"Time>=$splitTime\").cache()\n",
    "    println(\"Split data into Training and Validation: \")\n",
    "    println(\"training records count: \" + trainingData.count())\n",
    "    println(\"validation records count: \" + validationData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)-Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_38f606ec88d3\n",
      "bisectingkmeans: org.apache.spark.ml.clustering.BisectingKMeans = bisecting-kmeans_14b4abc832ce\n",
      "gaussianMixture: org.apache.spark.ml.clustering.GaussianMixture = GaussianMixture_6d798f46669a\n"
     ]
    }
   ],
   "source": [
    "// Create a Kmeans Model with K=2\n",
    "// train the model\n",
    "val kmeans = new KMeans().setK(2).setFeaturesCol(\"features\").setPredictionCol(\"clusters\").setSeed(1L).setMaxIter(100)\n",
    "val bisectingkmeans = new BisectingKMeans().setK(2).setFeaturesCol(\"features\").setPredictionCol(\"clusters\").setSeed(1L).setMaxIter(100)\n",
    "val gaussianMixture = new GaussianMixture().setK(2).setFeaturesCol(\"features\").setPredictionCol(\"clusters\").setSeed(1L).setMaxIter(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 545839927158490\n",
      "modelKmeans: org.apache.spark.ml.clustering.KMeansModel = kmeans_38f606ec88d3\n",
      "durationkmeans: Double = 4.842104795\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 4.842104795 secs\n"
     ]
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val t = System.nanoTime\n",
    "val modelKmeans = kmeans.fit(trainingData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 545847249076156\n",
      "modelBisectingkmeans: org.apache.spark.ml.clustering.BisectingKMeansModel = bisecting-kmeans_14b4abc832ce\n",
      "durationbisectingkmeans: Double = 27.809898236\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 27.809898236 secs\n"
     ]
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val t = System.nanoTime\n",
    "val modelBisectingkmeans = bisectingkmeans.fit(trainingData)\n",
    "val durationbisectingkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationbisectingkmeans secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 545876986190815\n",
      "modelGMM: org.apache.spark.ml.clustering.GaussianMixtureModel = GaussianMixture_6d798f46669a\n",
      "durationGMM: Double = 9.410226924\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 9.410226924 secs\n"
     ]
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val t = System.nanoTime\n",
    "val modelGMM = gaussianMixture.fit(trainingData)\n",
    "val durationGMM = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationGMM secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)- Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 545888353096704\n",
      "predictionsk: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 33 more fields]\n",
      "durationkmeans: Double = 0.275742023\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 0.275742023 secs\n",
      "predictionsk: org.apache.spark.sql.DataFrame = [label: double, clusters: double ... 1 more field]\n",
      "Classified test set :\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|  0.0|85544|\n",
      "|  1.0|  108|\n",
      "+-----+-----+\n",
      "\n",
      "Prediction :\n",
      "+--------+-----+\n",
      "|clusters|count|\n",
      "+--------+-----+\n",
      "|     0.0|37723|\n",
      "|     1.0|47929|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " // Make predictions Kmeans\n",
    "val t = System.nanoTime\n",
    "var predictionsk = modelKmeans.transform(validationData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")\n",
    "predictionsk = predictionsk.select(col(\"label\"),col(\"clusters\").cast(\"Double\"),col(\"features\"))\n",
    "//val df = raw.select(((1 to 28).map(i => \"V\" + i) ++ Array(\"Time\", \"Amount\", \"Class\")).map(s => col(s).cast(\"Double\")): _*)\n",
    "println(s\"Classified test set :\")\n",
    "validationData.groupBy(\"Class\").count().show()\n",
    "println(s\"Prediction :\")\n",
    "predictionsk.groupBy(\"clusters\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|clusters|label|count|\n",
      "+--------+-----+-----+\n",
      "|     0.0|  0.0|37664|\n",
      "|     1.0|  0.0|47880|\n",
      "|     0.0|  1.0|   59|\n",
      "|     1.0|  1.0|   49|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsk.select(\"clusters\", \"label\").groupBy(\"clusters\", \"label\").count().orderBy(\"label\", \"clusters\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 545892353507186\n",
      "predictionsbk: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 33 more fields]\n",
      "durationkmeans: Double = 0.272236378\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 0.272236378 secs\n",
      "predictionsbk: org.apache.spark.sql.DataFrame = [label: double, clusters: double ... 1 more field]\n",
      "Classified test set :\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|  0.0|85544|\n",
      "|  1.0|  108|\n",
      "+-----+-----+\n",
      "\n",
      "Prediction :\n",
      "+--------+-----+\n",
      "|clusters|count|\n",
      "+--------+-----+\n",
      "|     0.0|48498|\n",
      "|     1.0|37154|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " // Make predictions Bisecting Kmeans \n",
    "val t = System.nanoTime\n",
    "var predictionsbk = modelBisectingkmeans.transform(validationData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")\n",
    "predictionsbk = predictionsbk.select(col(\"label\"),col(\"clusters\").cast(\"Double\"),col(\"features\"))\n",
    "println(s\"Classified test set :\")\n",
    "validationData.groupBy(\"Class\").count().show()\n",
    "println(s\"Prediction :\")\n",
    "predictionsbk.groupBy(\"clusters\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|clusters|label|count|\n",
      "+--------+-----+-----+\n",
      "|     0.0|  0.0|48446|\n",
      "|     1.0|  0.0|37098|\n",
      "|     0.0|  1.0|   52|\n",
      "|     1.0|  1.0|   56|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsbk.select(\"clusters\", \"label\").groupBy(\"clusters\", \"label\").count().orderBy(\"label\", \"clusters\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: Long = 545896889141948\n",
      "predictionsgmm: org.apache.spark.sql.DataFrame = [V1: double, V2: double ... 34 more fields]\n",
      "durationkmeans: Double = 0.280427711\n",
      "\n",
      "initial model training finished.\n",
      "Training process takes 0.280427711 secs\n",
      "predictionsgmm: org.apache.spark.sql.DataFrame = [label: double, clusters: double ... 1 more field]\n",
      "Classified test set :\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|  0.0|85544|\n",
      "|  1.0|  108|\n",
      "+-----+-----+\n",
      "\n",
      "Prediction :\n",
      "+--------+-----+\n",
      "|clusters|count|\n",
      "+--------+-----+\n",
      "|     0.0|61381|\n",
      "|     1.0|24271|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Make predictions Bisecting Kmeans \n",
    "val t = System.nanoTime\n",
    "var predictionsgmm = modelGMM.transform(validationData)\n",
    "val durationkmeans = (System.nanoTime - t) / 1e9d\n",
    "println(\"\\ninitial model training finished.\")\n",
    "println(s\"Training process takes $durationkmeans secs\")\n",
    "predictionsgmm = predictionsgmm.select(col(\"label\"),col(\"clusters\").cast(\"Double\"),col(\"features\"))\n",
    "println(s\"Classified test set :\")\n",
    "validationData.groupBy(\"Class\").count().show()\n",
    "println(s\"Prediction :\")\n",
    "predictionsgmm.groupBy(\"clusters\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|clusters|label|count|\n",
      "+--------+-----+-----+\n",
      "|     0.0|  0.0|61306|\n",
      "|     1.0|  0.0|24238|\n",
      "|     0.0|  1.0|   75|\n",
      "|     1.0|  1.0|   33|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsgmm.select(\"clusters\", \"label\").groupBy(\"clusters\", \"label\").count().orderBy(\"label\", \"clusters\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatork1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_64913631471c\n",
      "evaluatork2: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_572879f9ced9\n",
      "evaluatork3: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_8f2a13d25235\n",
      "evaluatork4: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_f0033ef6bc30\n",
      "areaUnderROCk: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_f5afe190c8d4\n",
      "accuracyk: Double = 0.44030495493391864\n",
      "Area Under ROC Curve = 0.4469958713038298\n",
      "Accuracy = 0.44030495493391864\n",
      "Precision = 0.997178312387319\n",
      "Recall = 0.4403049549339187\n",
      "F1 = 0.6103282694480973\n",
      "Test Error = 0.5596950450660814\n"
     ]
    }
   ],
   "source": [
    "val evaluatork1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"accuracy\")\n",
    "val evaluatork2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedPrecision\")\n",
    "val evaluatork3 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedRecall\")\n",
    "val evaluatork4 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"f1\")\n",
    "val areaUnderROCk = new BinaryClassificationEvaluator().setRawPredictionCol(\"clusters\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val accuracyk = evaluatork1.evaluate(predictionsk)\n",
    "println(\"Area Under ROC Curve = \" + areaUnderROCk.evaluate(predictionsk))\n",
    "println(\"Accuracy = \" + evaluatork1.evaluate(predictionsk))\n",
    "println(\"Precision = \" + evaluatork2.evaluate(predictionsk))\n",
    "println(\"Recall = \" + evaluatork3.evaluate(predictionsk))\n",
    "println(\"F1 = \" + evaluatork4.evaluate(predictionsk))\n",
    "println(\"Test Error = \" + (1.0 - accuracyk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatorbk1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a3c05ea4a516\n",
      "evaluatorbk2: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_26b46dcdcc18\n",
      "evaluatorbk3: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_dd8cc7abf847\n",
      "evaluatorbk4: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_0aa78454c9e0\n",
      "areaUnderROCbk: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_37aebc8a7cfc\n",
      "accuracybk: Double = 0.5662681548591977\n",
      "Area Under ROC Curve = 0.542423478842164\n",
      "Accuracy = 0.5662681548591977\n",
      "Precision = 0.9976701270404935\n",
      "Recall = 0.5662681548591977\n",
      "F1 = 0.7219404016581634\n",
      "Test Error = 0.4337318451408023\n"
     ]
    }
   ],
   "source": [
    "val evaluatorbk1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"accuracy\")\n",
    "val evaluatorbk2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedPrecision\")\n",
    "val evaluatorbk3 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedRecall\")\n",
    "val evaluatorbk4 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"f1\")\n",
    "val areaUnderROCbk = new BinaryClassificationEvaluator().setRawPredictionCol(\"clusters\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val accuracybk = evaluatorbk1.evaluate(predictionsbk)\n",
    "println(\"Area Under ROC Curve = \" + areaUnderROCbk.evaluate(predictionsbk))\n",
    "println(\"Accuracy = \" + evaluatorbk1.evaluate(predictionsbk))\n",
    "println(\"Precision = \" + evaluatorbk2.evaluate(predictionsbk))\n",
    "println(\"Recall = \" + evaluatorbk3.evaluate(predictionsbk))\n",
    "println(\"F1 = \" + evaluatorbk4.evaluate(predictionsbk))\n",
    "println(\"Test Error = \" + (1.0 - accuracybk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluatorgmm1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_1b1f2648b35d\n",
      "evaluatorgmm2: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_355ae279caff\n",
      "evaluatorgmm3: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e174d3b8f0ad\n",
      "evaluatorgmm4: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_5ad2c85f083d\n",
      "areaUnderROCgmm: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_e037b2dd8c96\n",
      "accuracygmm: Double = 0.7161420632326158\n",
      "Area Under ROC Curve = 0.5111079938069557\n",
      "Accuracy = 0.7161420632326158\n",
      "Precision = 0.9975204623430861\n",
      "Recall = 0.7161420632326158\n",
      "F1 = 0.8334721666150667\n",
      "Test Error = 0.28385793676738424\n"
     ]
    }
   ],
   "source": [
    "val evaluatorgmm1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"accuracy\")\n",
    "val evaluatorgmm2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedPrecision\")\n",
    "val evaluatorgmm3 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"weightedRecall\")\n",
    "val evaluatorgmm4 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"clusters\").setMetricName(\"f1\")\n",
    "val areaUnderROCgmm = new BinaryClassificationEvaluator().setRawPredictionCol(\"clusters\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "val accuracygmm = evaluatorgmm1.evaluate(predictionsgmm)\n",
    "println(\"Area Under ROC Curve = \" + areaUnderROCgmm.evaluate(predictionsgmm))\n",
    "println(\"Accuracy = \" + evaluatorgmm1.evaluate(predictionsgmm))\n",
    "println(\"Precision = \" + evaluatorgmm2.evaluate(predictionsgmm))\n",
    "println(\"Recall = \" + evaluatorgmm3.evaluate(predictionsgmm))\n",
    "println(\"F1 = \" + evaluatorgmm4.evaluate(predictionsgmm))\n",
    "println(\"Test Error = \" + (1.0 - accuracygmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
